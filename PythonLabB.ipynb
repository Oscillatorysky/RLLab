{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(3,2)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        count = 0\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            count += 1\n",
    "            #print(count)\n",
    "        return V\n",
    "    \n",
    "    def optimal_policy(self, policy, threshold, discount):\n",
    "        V = self.policy_evaluation(policy, threshold, discount) \n",
    "        P = np.zeros((self.state_size, self.action_size))\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                \n",
    "                loc = (i,j)\n",
    "                if(not self.is_location(loc)):\n",
    "                    continue\n",
    "                state_idx = self.loc_to_state(loc, locations)\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                    \n",
    "                nindir = self.get_neighbour(loc, 'nr')\n",
    "                state_prime = self.loc_to_state(nindir, locations)\n",
    "                dir = 0\n",
    "                optdir = dir\n",
    "                nlocqmax = T[state_prime, state_idx, dir] * (R[state_prime,state_idx, dir] + discount * V[state_prime])\n",
    "                for direction in ['ea','so', 'we']:\n",
    "                    nindir = self.get_neighbour(loc,direction)\n",
    "                    state_prime = self.loc_to_state(nindir, locations)\n",
    "                    if(direction == 'ea'):\n",
    "                        dir = 1\n",
    "                    elif(direction == 'so'):\n",
    "                        dir = 2\n",
    "                    else:\n",
    "                        dir = 3\n",
    "                    tmpQ = T[state_prime, state_idx, dir] * (R[state_prime,state_idx, dir] + discount * V[state_prime])\n",
    "                    if(nlocqmax < tmpQ):\n",
    "                        nlocqmax = tmpQ\n",
    "                        optdir = dir\n",
    "                        \n",
    "                \n",
    "                #choose optimal policy for that state\n",
    "                P[state_idx, optdir] = 1;\n",
    "                     \n",
    "        return P\n",
    "    \n",
    "    def policy_iteration(self, policy, threshold, discount, iterlimit):\n",
    "        Optpolicy = np.zeros((self.state_size, self.action_size))\n",
    "        E = policy - Optpolicy\n",
    "        count = 0\n",
    "        while(not ((E == np.zeros((self.state_size, self.action_size))).all())):\n",
    "            Optpolicy = grid.optimal_policy(policy, 0.05, 1)\n",
    "            E = policy - Optpolicy\n",
    "            policy = OptPolicy\n",
    "            count += 1\n",
    "            if(count > iterlimit):\n",
    "                break\n",
    "        return Optpolicy\n",
    "        \n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "        \n",
    "    def policy_transfer(self,policy):\n",
    "        Propolicy = np.zeros(policy.shape[0]).astype(int)\n",
    "        for state_idx in range(policy.shape[0]):\n",
    "            for dir in range(policy.shape[1]):\n",
    "                if(policy[state_idx,dir] == 1):\n",
    "                    Propolicy[state_idx] = dir\n",
    "        \n",
    "        return Propolicy\n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is : [-3.79227299 -2.48898229 -1.36125106 -0.78730193 -0.42775228 -5.28625442\n",
      " -0.94470708 -0.6868109  -0.17044571 -6.95541021 -0.89686394 -0.94190259\n",
      "  0.51485864 -8.75292799 -9.3605402  -2.77149013  2.59218872  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))+0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val = grid.policy_evaluation(Policy, 0.05, 1) #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of that policy is : [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "The value of that policy is : [ 0.19325362  0.84223772  1.1255818   2.87740052  3.78045604  0.13172971\n",
      "  1.31131712  3.88530469  5.07534998  0.08558628  1.41616577  5.09057333\n",
      "  6.8102959  -0.05073573 -1.13139432  5.26547582  9.15351572  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Optimal Policy Base on Policy Iteration\n",
    "grid = GridWorld()\n",
    "\n",
    "Threshold = 0.02\n",
    "Discount = 0.8\n",
    "IterLimit = 100\n",
    "\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))+0.25\n",
    "OptPolicy = grid.policy_iteration(Policy, Threshold, Discount, IterLimit)\n",
    "print(\"The value of that policy is : {}\".format(OptPolicy))\n",
    "\n",
    "val = grid.policy_evaluation(OptPolicy, Threshold, Discount) #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n",
    "\n",
    "OptPolicyShow = grid.policy_transfer(OptPolicy)\n",
    "grid.draw_deterministic_policy(OptPolicyShow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
